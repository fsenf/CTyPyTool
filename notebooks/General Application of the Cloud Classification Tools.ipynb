{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "national-polyester",
   "metadata": {},
   "source": [
    "#### This is only needed during development\n",
    "\n",
    "The path for the classifier is set here as well as modules reloaded in case they have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "northern-brook",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_handler' from '../cloud_classifier/data_handler.py'>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the system \n",
    "import sys\n",
    "sys.path.append('../cloud_classifier')\n",
    "\n",
    "import cloud_trainer \n",
    "import data_handler\n",
    "import importlib\n",
    "importlib.reload(cloud_trainer)\n",
    "importlib.reload(data_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-plaza",
   "metadata": {},
   "source": [
    "# General Application of the Cloud Classification Tools\n",
    "\n",
    "This is a cloud classifier description text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-massachusetts",
   "metadata": {},
   "source": [
    "# Data Extraction\n",
    "\n",
    "The first step of any maschine learning routine is the obtaining of data in a usable form.\n",
    "The satelite and label data used in this project consists of satelite data stored in the `netcdf` data format. \n",
    "\n",
    "The Class `data_handler` helps to extract trainning vectors from this data which can be used in later steps to train a classifer for automatic cloud type detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-favor",
   "metadata": {},
   "source": [
    "#### Adding data sets to the data handler\n",
    "\n",
    "In a first step the locations of a number of netcdf data files consisting of satelite data and according label data are added to the `data_handler` via the `add_trainig_files` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "married-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_handler\n",
    "\n",
    "data_1 = \"../data/msevi-nawdex-20160920.nc\"\n",
    "labels_1 = \"../data/nwcsaf_msevi-nawdex-20160920.nc\"\n",
    "data_2 = \"../data/msevi-nawdex-20160925.nc\"\n",
    "labels_2 = \"../data/nwcsaf_msevi-nawdex-20160925.nc\"\n",
    "\n",
    "dh = data_handler.data_handler()\n",
    "\n",
    "dh.add_training_files(data_1, labels_1)\n",
    "dh.add_training_files(data_2, labels_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-telescope",
   "metadata": {},
   "source": [
    "#### Using a mask file to narrow down the data\n",
    "\n",
    "Netcdf datasets may contain data for large swath of land and ocean. In order to limit the scope of the data we can use a mask from a h5-mask-file. The `set_indices_from_mask` method allows instructing the `data_handler` to only use data from inside a sepcific region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "several-habitat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([316, 316, 316, ..., 714, 714, 714]),\n",
       " array([1904, 1905, 1906, ..., 2169, 2170, 2171]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = \"../data/region_masks_for_msevi_nawdex.h5\"\n",
    "dh.set_indices_from_mask(mask, \"mediterranean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-junction",
   "metadata": {},
   "source": [
    "#### Setting parameters for the data extraction\n",
    "\n",
    "A number of parameters for the data extraction can be set. The method `set_extraction_parameters` allows specifing the number of samples per data set, the hours at which the dataset is sampled and a choice of creating difference vectors from the extraced data vectors.\n",
    "\n",
    "The `set_nwcsaf_version` mehtod allows to specify which nwcsaf standard for mapping cloud types is used and the `set_input_channels` method allows to name the data channels from the sattelite data that are to be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "declared-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "dh.set_extraction_parameters(samples = 1000, hours = range(24), difference_vectors = True, original_values = True)\n",
    "dh.set_nwcsaf_version(in_version = \"auto\", out_version = \"v2018\")\n",
    "dh.set_input_channels(input_channels = ['bt062', 'bt073', 'bt087', 'bt097', 'bt108', 'bt120', 'bt134'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-dance",
   "metadata": {},
   "source": [
    "#### Extracting data\n",
    "\n",
    "Based on the above specification a set of feature vectors and labels can now be extracted from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "sophisticated-finance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cloud type data is coded after the old (2013) standard\n",
      "nwcsaf-in-version set accordingly\n"
     ]
    }
   ],
   "source": [
    "feature_vectors, labels = dh.create_training_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-association",
   "metadata": {},
   "source": [
    "# Training Classifer\n",
    "\n",
    "The class `cloud_trainer` wraps a (small) selection of the machine learning functionality from `scikitlearn` for easier use in this specialized scenario. When the need for other mehtods or greater amount of finetuning is arises, the data extracted above can be used with `scikitlearn` directly.\n",
    "\n",
    "#### Creating a cloud classifier \n",
    "\n",
    "After setting some parameters like the type of classifer and if feature preselection is performed the `cloud_trainer` can be trained with the training data created above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "floppy-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloud_trainer\n",
    "\n",
    "ct = cloud_trainer.cloud_trainer()\n",
    "\n",
    "ct.set_training_paremeters(classifer_type = \"Forest\", feature_preselection = False, max_depth = 20)\n",
    "ct.train_classifier(feature_vectors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-hartford",
   "metadata": {},
   "source": [
    "#### Evaluating Classifier and  Predicting labels \n",
    "\n",
    "The classifier can now be used to predict labels for a new set of satelite data.\n",
    "The `data_handler` is used again to extract the feature vectors over this new data set and also returns the indices of the created vectors. Those are necessarry to map the feature vectors as well as later the predicted labels to their specific positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "stretch-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3 = \"../data/msevi-nawdex-20160930.nc\"\n",
    "test_vectors, test_indices = dh.create_test_vectors(data_3, hour = 0)\n",
    "\n",
    "predicted_labels = ct.predict_labels(test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-trust",
   "metadata": {},
   "source": [
    "Alternativly, when label data for this new set of satelite data set also exists the performance of the classifer over this data can then be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "shaped-catch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly identified 153161 out of 171396 labels! \n",
      "Positve rate is: 0.893609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8936089523676165"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_3 = \"../data/nwcsaf_msevi-nawdex-20160930.nc\"\n",
    "\n",
    "test_labels = dh.extract_labels(labels_3, test_indices, hour = 0)\n",
    "ct.evaluate_classifier(test_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-scholar",
   "metadata": {},
   "source": [
    "Finally, when only wanting to evaluate the selected training methods and parameters the method `evaluate_parameters` will split a set of feature vectors and labels for a quick evaluation withouding needing any new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "armed-violin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly identified 10527 out of 12000 labels! \n",
      "Positve rate is: 0.877250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.87725"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.evaluate_parameters(feature_vectors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-vermont",
   "metadata": {},
   "source": [
    "# Saving and Loading\n",
    "\n",
    "Some of the steps from extracting data until training the classifier are relativly resource intensive and take some time to finish, with this time increasing with the size of the used datasets.\n",
    "Therefore a few methods exist to save and load time consuming intermiediates and the finished classifier itself.\n",
    "\n",
    "#### Saving and Loading feature vectors\n",
    "One of the more time consuming steps is the creation of large sets of feature vectors and corresponding labels from the data sets. The methods `save_training_set` and `load_training_set` can help to save computation time when testing different training parameters over the same set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ancient-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../training_set_1\"\n",
    "\n",
    "dh.save_training_set(feature_vectors, labels, filename)\n",
    "\n",
    "feature_vectors, labels = dh.load_training_set(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-distribution",
   "metadata": {},
   "source": [
    "#### Saving and Loading the classifier\n",
    "In order to not having to train the classifier each time it is used it can also be stored and loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "tight-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_cl = \"../classifer_1\"\n",
    "\n",
    "ct.save_classifier(filename_cl)\n",
    "\n",
    "ct.load_classifier(filename_cl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-locator",
   "metadata": {},
   "source": [
    "#### Exporting labels into NETCDF format\n",
    "The method `make_xrData` usess a refernce NETCDF file in order to write the predicted labels into the coorect coordiantes of an xarray dataset. If an output file is specified this method will save the labels as a NETCDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "mechanical-roots",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No refrence file given!\n",
      "Using latest test file as reference\n"
     ]
    }
   ],
   "source": [
    "labels_xr = dh.make_xrData(predicted_labels, test_indices, NETCDF_out = \"../predicted_labels_1.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-transsexual",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
